#### configuration and directory
CONFIG_NAME: Pretrain
dataset_name: celeba
data_dir: ./data/celeba
checkpoints_path: ./checkpoints
test_pair_list: ./data/celeba/celeba/celeba_test_10_pair.txt  
resume_model_path: ./checkpoints/celeba/Pretrain/Bert/arcface_text_encoder_bert_first_.pth
resume_epoch: 1

# machine setup
num_workers: 6 
gpu_id: [0, 1]
manual_seed: 100
CUDA: True

is_DAMSM: True  
is_CLIP: False  
is_CMP: False      

lambda_cl: 2.0
aux_feat_dim_per_granularity: 64

# model arch 
img_size: 128  #299 for Inception
lr_text_bert:  0.0001  #(1e-4) initial learnig rate 
lr_text: 0.0003
lr_image: 0.0001
#lr_image : 0.0001 #(1e-4)
lr_head: 0.003
weight_decay: 0.01 #(1e-3)
clip_max_norm: 1.0


#trainng settings
batch_size: 16 #16 for first step 
max_epoch: 8
test_interval: 1
save_interval: 1
patience: 1 # 1 for BERT
factor: 0.8
temperature: 1.0

# flags
do_test: False   
prev_weight: False 

# encoder
TRAIN:
    FLAG: True
    SMOOTH:
        GAMMA1: 4.0  
        GAMMA2: 5.0
        GAMMA3: 10.0

using_BERT: True      
rnn_type: LSTM #GRU
en_type: "bert"   
bert_words_num: 21
lstm_words_num: 18 
embedding_dim: 256
captions_per_image: 10
bert_config:  bert-base-uncased #distilbert-base-uncased 

#for inception image encodeer
#1. chnage img_size = 299
#2. get_imgs Image.open().to("RGB")