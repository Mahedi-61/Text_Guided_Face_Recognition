#### configuration and directory
CONFIG_NAME: Pretrain
dataset_name: celeba
data_dir: ./data/celeba
checkpoints_path: ./checkpoints
test_pair_list: ./data/celeba/celeba/celeba_valid_pair.txt  
resume_model_path: "./checkpoints/celeba/Pretrain/adaface/Bert/adaface_text_encoder_bert_5.pth"
resume_epoch: 1

# machine setup
num_workers: 6 
gpu_id: [0, 1]
manual_seed: 100
CUDA: True

# losses
is_DAMSM: True  
is_CLIP: False  
is_CMP: False      
lambda_cl: 2.0


# model arch 
aux_feat_dim_per_granularity: 64 
img_size: 112 #for adaface #*************CHANGE
model_type: adaface        #*************CHANGE
ch_size: 3                 #*************CHANGE

lr_text_bert:  0.00007  #(7e-5) initial learnig rate 
init_lr_text: 0.0003
min_lr_text: 0.00001 #(1e-4 for RNN and 1e-5 for BERT) *************CHANGE
r_step: 2000
lr_head: 0.002
weight_decay: 0.01 #(1e-3) for lstm******************** CHANGE 0.01 for BERT
clip_max_norm: 1.0 #0.25 for lstm ****************CHANGE


#trainng settings
batch_size: 32 #16 for first step *****************CHANGE
max_epoch: 20 
test_interval: 1
save_interval: 1
patience: 1 # 8 for lstm 1 for BERT
factor: 0.8
temperature: 1.0

# flags
do_test: False   

# encoder
TRAIN:
    FLAG: True
    SMOOTH:
        GAMMA1: 4.0  
        GAMMA2: 5.0
        GAMMA3: 10.0

using_BERT: True      
rnn_type: LSTM #GRU
en_type: "bert"   
bert_words_num: 21
lstm_words_num: 18 
embedding_dim: 256
captions_per_image: 10
bert_config:  bert-base-uncased #distilbert-base-uncased 