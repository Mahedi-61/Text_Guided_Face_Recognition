CONFIG_NAME: Fusion
dataset_name: celeba
data_dir: ./data/celeba
test_pair_list: ./data/celeba/celeba/celeba_valid_pair.txt  
checkpoints_path: ./checkpoints
load_model_path: ./weights/celeba/FE/resnet18_celeba_110.pth
resume_model_path: ./weights/celeba/state_epoch_lstm_linear_009.pth
text_encoder_path: ./weights/celeba/Pretrain/Bert/arcface_text_encoder_bert_second_10.pth
resume_epoch: 1

# machine setup 
num_workers: 4 
gpu_id: [0, 1]
manual_seed: 100
cuda: True

# model arch
img_size: 128
ch_size: 1
backbone: resnet18
classify: softmax
num_classes: 24000 
metric: arc_margin 
easy_margin: False
loss: focal_loss 
optimizer: sgd 
use_se: False

# training settings 
lr_image_train: 0.05   #0.05 for linear fusion 
lr_step: 5
gamma: 0.5  
weight_decay: 0.0005
max_epoch: 12 
batch_size: 32 
test_interval: 1
save_interval: 1
temperature: 1.0
trainable: False

# fusion arch
fusion_type: concat #concat_attention #concat, linear, sentence_attention #cross_attention 
fusion_final_dim: 768
clip_loss: True 

# flags
prev_weight: False  
do_test: True
is_second_step: False 

# encoder settings
rnn_type: LSTM #GRU 
using_BERT: True 
en_type: "bert"     
bert_words_num: 21
lstm_words_num: 18 
bert_config:  bert-base-uncased #distilbert-base-uncased  
embedding_dim: 256
captions_per_image: 10